{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding\n",
    "When training some machine learning task, normally one has to provide a dataset to provide access to the samples in the training data. In a default case we need to have some input features and some targets. With audiomate such data is stored in a Container (for targets) or a FeatureContainer (for input features). The feeding module provides wrappers around containers as dataset-like objects, as they are often required by some machine/deep-learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import audiomate\n",
    "from audiomate import corpus\n",
    "from audiomate.corpus import assets\n",
    "from audiomate.utils import units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbansound8k_subset = audiomate.Corpus.load('data/urbansound_subset', reader='urbansound8k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features/targets\n",
    "First we need to extract the features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_settings = units.FrameSettings(2048, 1024)\n",
    "sampling_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomate.processing import pipeline\n",
    "\n",
    "feature_path = 'output/mel_features.hdf5'\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "mel_extractor = pipeline.MelSpectrogram(n_mels=23)\n",
    "power_to_db = pipeline.PowerToDb(ref=np.max, parent=mel_extractor)\n",
    "\n",
    "features = power_to_db.process_corpus(urbansound8k_subset, \n",
    "                                feature_path, \n",
    "                                frame_size=frame_settings.frame_size, \n",
    "                                hop_size=frame_settings.hop_size, \n",
    "                                sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomate import encoding\n",
    "\n",
    "target_path = 'output/targets.hdf5'\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "labels = list(urbansound8k_subset.all_label_values(corpus.LL_SOUND_CLASS))\n",
    "encoder = encoding.FrameHotEncoder(labels, corpus.LL_SOUND_CLASS, frame_settings, sampling_rate)\n",
    "encodings = encoder.encode_corpus(urbansound8k_subset, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding single frames\n",
    "In this scenario a single training sample consists of one frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ -7.2746153,  -0.8263799,  -3.2943037,  -5.7100906,  -8.060489 ,\n",
       "         -7.1336184, -11.988809 , -15.724296 , -12.836285 , -15.029507 ,\n",
       "        -14.673418 , -15.978542 , -13.3668995, -12.036485 , -13.558588 ,\n",
       "        -14.553731 , -14.105342 , -18.139305 , -16.73909  , -16.591326 ,\n",
       "        -13.479118 , -15.261944 , -17.350136 ], dtype=float32),\n",
       " array([1., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audiomate import feeding\n",
    "\n",
    "input_container = assets.FeatureContainer('output/mel_features.hdf5')\n",
    "target_container = assets.Container('output/targets.hdf5')\n",
    "\n",
    "input_container.open()\n",
    "target_container.open()\n",
    "\n",
    "single_frame_dataset = feeding.FrameDataset(urbansound8k_subset, [input_container, target_container])\n",
    "\n",
    "# Get a single sample, which is a tuple/list with input/target for the frame 3\n",
    "single_frame_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since index access to single frames of the underlying hdf5-files (via h5py) is quite slow, we can use a PartitioningIterator, that loads partitions of a given maximal size into memory. After all frames are iterate over, the next partition is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-50.76419 , -45.63187 , -33.773666, -31.817165, -40.481903,\n",
       "        -47.343697, -47.572548, -46.96832 , -50.65575 , -46.23703 ,\n",
       "        -48.19282 , -49.336872, -48.22521 , -46.70622 , -50.95082 ,\n",
       "        -59.49267 , -60.997467, -65.7069  , -64.58801 , -64.83406 ,\n",
       "        -68.44708 , -68.750336, -69.54637 ], dtype=float32),\n",
       " array([0., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create via dataset\n",
    "single_frame_iterator = single_frame_dataset.partitioned_iterator('250M', shuffle=True, seed=34)\n",
    "\n",
    "# Or direct\n",
    "single_frame_iterator = feeding.FrameIterator(urbansound8k_subset, [input_container, target_container], \n",
    "                                              '200M', shuffle=True, seed=23)\n",
    "\n",
    "next(single_frame_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Multiple Frames\n",
    "In some cases a single training sample should be a sequence of frames. In this case the MultiFrameDataset can be used, which returns an array of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape of sample: (4, 23)\n",
      "Target shape of sample: (4, 23)\n",
      "Number of frames in sample: 4\n"
     ]
    }
   ],
   "source": [
    "from audiomate import feeding\n",
    "\n",
    "input_container = assets.FeatureContainer('output/mel_features.hdf5')\n",
    "target_container = assets.Container('output/targets.hdf5')\n",
    "\n",
    "input_container.open()\n",
    "target_container.open()\n",
    "\n",
    "multi_frame_dataset = feeding.MultiFrameDataset(urbansound8k_subset, [input_container, target_container], \n",
    "                                               frames_per_chunk=4, return_length=True, pad=True)\n",
    "\n",
    "# Get a single sample, which is a tuple/list with input/target chunk with index 3\n",
    "sample = multi_frame_dataset[3]\n",
    "\n",
    "print('Input feature shape of sample: {}'.format(str(sample[0].shape)))\n",
    "print('Target shape of sample: {}'.format(str(sample[0].shape)))\n",
    "print('Number of frames in sample: {}'.format(sample[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the FrameDataset, for the MultiFrame case a partitioned iterator can be used in the same manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding full Utterances\n",
    "For example for speech recognition our training samples are most likely full utterances. Furthermore the dimension of inputs and targets can differ. With the UtteranceDataset we can load full utterances as samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (62, 23)\n",
      "Length of the input features: 62\n",
      "Targets shape: (62, 2)\n",
      "Length of the targets: 62\n"
     ]
    }
   ],
   "source": [
    "from audiomate import feeding\n",
    "\n",
    "input_container = assets.FeatureContainer('output/mel_features.hdf5')\n",
    "target_container = assets.Container('output/targets.hdf5')\n",
    "\n",
    "input_container.open()\n",
    "target_container.open()\n",
    "\n",
    "utt_dataset = feeding.UtteranceDataset(urbansound8k_subset, [input_container, target_container], pad=True)\n",
    "\n",
    "# Get a sample (input-feats-utt-1, len-input-feats-utt-1, targets-utt-1, len-target-utt-1)\n",
    "sample = utt_dataset[1]\n",
    "print('Input feature shape: {}'.format(str(sample[0].shape)))\n",
    "print('Length of the input features: {}'.format(sample[1]))\n",
    "print('Targets shape: {}'.format(str(sample[2].shape)))\n",
    "print('Length of the targets: {}'.format(sample[3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
